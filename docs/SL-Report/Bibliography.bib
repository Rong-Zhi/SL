%% Created using Papers on Tue, 17 Jan 2012.
%% http://mekentosj.com/papers/

@article{Barto:2003uh,
author = {Barto, AG},
title = {{Recent Advances in Hierarchical Reinforcement Learning
}},
journal = {Discrete Event Dynamic Systems},
year = {2003}
}

@article{Andrieu:2003wb,
author = {Andrieu, C and De Freitas, N and Doucet, A},
title = {{An introduction to MCMC for machine learning}},
journal = {Machine Learning},
year = {2003}
}

@article{Taylor:2011vr,
author = {Taylor, ME},
title = {{An Introduction to Inter-task Transfer for Reinforcement Learning}},
journal = {AI Magazine},
year = {2011}
}

@article{Sutton:2000uo,
author = {Sutton, RS and McAllester, D and Singh, S},
title = {{Policy gradient methods for reinforcement learning with function approximation}},
journal = {Advances in Neural Information Processing Systems (NIPS)},
year = {2000}
}

@article{Grollman:2011vu,
author = {Grollman, DH},
title = {{Donut as I do: Learning from failed demonstrations}},
journal = {International Conference on Robotics and {\ldots}},
year = {2011}
}

@article{Peters:2003ue,
author = {Peters, J and Vijayakumar, S},
title = {{Reinforcement learning for humanoid robotics}},
journal = {{\ldots} conference on humanoid robots},
year = {2003}
}

@article{Schaal:2003ic,
author = {Schaal, S and Ijspeert, A and BILLARD, A},
title = {{Computational approaches to motor learning by imitation}},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
year = {2003},
volume = {358},
number = {1431},
pages = {537--547},
month = mar
}

@article{Cevher:2008we,
author = {Cevher, Volkan},
title = {{EXPECTATION PROPAGATION}},
year = {2008},
pages = {1--9},
month = dec
}

@booklet{Smola:2002ta,
title = {{Learning with kernels}},
author = {Smola, AJ},
year = {2002}
}

@article{Calinon:2007wh,
author = {Calinon, S and Guenter, F},
title = {{On learning, representing, and generalizing a task in a humanoid robot}},
journal = {Systems},
year = {2007}
}

@booklet{Jaynes:2003ua,
title = {{Probability theory: the logic of science}},
author = {Jaynes, ET},
year = {2003}
}

@article{Kolber:2009uk,
author = {Kober, Jens},
title = {{Reinforcement Learning for Motor Primitives}},
year = {2009},
pages = {1--89},
month = feb,
note = {- Combined use of framework for motor primitives and reduction of reward-related self-improvement to reward-weighted regression
- Use of EM style algo instead of gradient search
- Extension of motor primitive framework to allow for vision coupling 


}
}

@article{Sung:2004wq,
author = {Sung, HG},
title = {{Gaussian mixture regression and classification}},
year = {2004}
}

@article{Pan:dm,
author = {Pan, Sinno Jialin and Yang, Qiang},
title = {{A Survey on Transfer Learning}},
journal = {IEEE Transactions on Knowledge and Data Engineering},
volume = {22},
number = {10},
pages = {1345--1359},
note = {- 3 main issues: 
	- What to transfer
	- How to transfer
	- When to transfer
- 3 categorizations:
	1) Inductive transfer learning:
		- Tasks are different
		- Domains may be the same
		- Requires some labeled data in the target domain to induce an objective predictive model f(.)
	2) Transductive transfer learning
		- Tasks are the same
		- Domains are different
		- No labeled data in target domain, lots of data in source domain
	3) Unsupervised transfer learning
		- Tasks are different but related
		- No labeled data is available on either domain
- What to transfer:
	1{\_} Instance based transfer learning, transfer certain parts of data of source domain to target domain by reweighing
	2{\_} feature representation transfer approach, learn good feature representation for the target domain
	3{\_} Parameter transfer approach. Shared parameters or priors
	4{\_} Relational knowledge transfer. Some relation between source and target domain.

- Inductive:
	- Transfer Instances:
		- TrAdaBoost: 
		- Jiang {\&}amp; Zhai: remove misleading examples
		- Liao: label target domain data with help of source
		- Wu {\&}amp; Dietterich: integrate source domain data as SVM
	- Transfer Feature Representations:
		- Learn common features by optimization problem
		- Raina: learn higher level features
	- Transfer Parameters:
		- Lawrence {\&}amp; Platt: Share prior for Gaussian Processes
		- Evgeniou {\&}amp; Pontil: Use SVMs with broken up param w
	- Transfer Relational Knowledge:
		- Mihalkova: TAMAR, find mapping between domains

-Transductive:
	- Transfer Instances:

- 2 Contests :
	- ECML/PKDD 2006
	- ICDM 2007
- UC Berkeley Matlab toolbox for transfer learning
- Future Research:
	- Avoid negative transfer
	- Find similarity metric between domains / tasks
	- Different feature spaces
	- Transfer form multiple domains (heterogeneous transfer learning)
	

}
}

@article{Kober:2010vx,
author = {Kober, J and Oztop, E},
title = {{Reinforcement learning to adjust robot movements to new situations}},
journal = {{\ldots} of robotics: science and systems},
year = {2010}
}

@article{Kober:2010uv,
author = {Kober, J and M{\"u}lling, K and Kromer, O},
title = {{Movement templates for learning of hitting and batting}},
journal = {Proceedings of the Robotics and Automation (ICRA)},
year = {2010}
}

@booklet{Jaynes:2003uaa,
title = {{Probability theory: the logic of science}},
author = {Jaynes, ET},
year = {2003},
note = {Best paper eva}
}

@article{Kober:2008ua,
author = {Kober, J and Mohler, B},
title = {{Learning perceptual coupling for motor primitives}},
journal = {Intelligent Robots and Systems},
year = {2008}
}

@inproceedings{Peters:2010wp,
author = {Peters, J and M{\"u}lling, K},
title = {{Relative entropy policy search}},
booktitle = {Proceedings of the 24th National Conference on Artificial Intelligence (AAAI)},
year = {2010}
}

@article{Bischoff:2010wz,
author = {Bischoff, R and Kurth, J and Schreiber, G},
title = {{The KUKA-DLR Lightweight Robot arm-a new reference platform for robotics research and manufacturing}},
journal = {Robotics (ISR)},
year = {2010}
}

@article{Taylor:2009ur,
author = {Taylor, ME},
title = {{Transfer learning for reinforcement learning domains: A survey}},
journal = {The Journal of Machine Learning Research},
year = {2009},
note = {- 3 distinct tasks : 
	1) Select appropriate source task
	2) Learn relation to target task
	3) Transfer knowledge
- Difference total time scenario/ target task time scenario
- Different performance metrics:
	Jumpstart
	Asymptotic perf.
	Total Reward
	Transfer Ratio
	Time to Threshold
- 5 Dimensions of TL:
	1) Task difference assumption
	2) Source taks selection
		Guided by humans, one or more source tasks, guard against negative transfer
	3) Task Mappings
		Given by human, online/offline learning
	4) Transferred Knowledge
		High/low level info, info about how to learn
	5) Allowed Learners
		Select diff algo based on task
- Related paradigms: Imitation learning, human advice, reward shaping
- Transfer methods for Fixed State Vars and Actions:
	- Selfridge: Pole balancing, making task harder by using longer poles
	- Asada: Incremental learning of maze
	- Atkeson and Santamaria: transfer btw tasks with diff in reward func
	- Asadi {\&}amp; Huber: decision level model + evaluation level model -{\&}gt; Hierarchical Bounded Parameter SMDP
	- Andre {\&}amp; Russel: copy local policies directly
	- Ravindran {\&}amp; Barto: relativized options
	- Ferguson {\&}amp; Mahadevan: Proto-Value Functions
	- Sherstov {\&}amp; Stone: Transfer reduced action set
	-! Lazaric: Transfer source task instances or soure regions 
- Multi Taks Learning Methods
	- Choose from one or more source tasks
	- Mehta: Variable reward hierarchical reinforcement learning
	- Sunmola {\&}amp; Wyatt: use instances from source task to set new prior
	- Wilson: Transfer prior from previous task
	- Lazaric: Learn tasks in parallel 
- Transferring Task-Invariant Knowledge Between Tasks with Differring State Variables and Actions
	- Konidaris {\&}amp; Barto: Problem space and agent space
	}
}

@article{BILLARD:2004js,
author = {BILLARD, A},
title = {{Discovering optimal imitation strategies}},
journal = {Robotics and Autonomous Systems},
year = {2004},
volume = {47},
number = {2-3},
pages = {69--77},
month = jun
}

@article{Peters:2009ud,
author = {Peters, Jan},
title = {{Deriving LQR from a Stochastic Policy Point of View}},
year = {2009},
pages = {1--17},
month = jul
}

@article{Peters:2007vv,
author = {Peters, J},
title = {{Reinforcement learning by reward-weighted regression for operational space control}},
journal = {{\ldots} international conference on Machine learning},
year = {2007}
}

@article{Deisenroth:tp,
author = {Deisenroth, MP},
title = {{Multiple-Target Reinforcement Learning with a Single Policy}},
journal = {mit.edu}
}

@article{Kober:2008us,
author = {Kober, J},
title = {{Policy search for motor primitives in robotics}},
journal = {Machine Learning},
year = {2008}
}

@article{Taylor:2007vn,
author = {Taylor, Matthew E. and Stone, Peter and Liu, Yaxin},
title = {{Transfer learning via inter-task mappings for temporal difference learning}},
journal = {Journal of Machine Learning Research},
year = {2007},
volume = {8},
pages = {2125--2167}
}

@article{Hinton:2002wb,
author = {Hinton, GE},
title = {{Training products of experts by minimizing contrastive divergence}},
journal = {Neural Computation},
year = {2002}
}

@article{Schaal:1999ud,
author = {Schaal, S},
title = {{Is imitation learning the route to humanoid robots}},
journal = {Trends in cognitive sciences},
year = {1999}
}

@article{Barrett:2010vv,
author = {Barrett, S and Taylor, ME},
title = {{Transfer learning for reinforcement learning on a physical robot}},
journal = {{\ldots} Learning Agents Workshop (AAMAS- {\ldots}},
year = {2010}
}

@article{Kober:2010vxa,
author = {Kober, J and Oztop, E},
title = {{Reinforcement learning to adjust robot movements to new situations}},
journal = {{\ldots} of robotics: science and systems},
year = {2010}
}

@article{Sternad:2010wd,
author = {Sternad, D and Park, SW and M{\"u}ller, H},
title = {{Coordinate Dependence of Variability Analysis}},
journal = {PLoS Computational Biology},
year = {2010},
note = {- Tetherball setup
- Analysis of performance development 
- Analysis of impact of choice of coordinate system on results of analyzing covariance matrix.
- Diff error sources:
	- T: Finding the region of least sensitivity in the execution space Tolerance Cost
	- N: Random deviations in performance Sensorimotor Noise Cost
	- C: Explotiing covariation between execution variables

- Meaningful representations of data can be transformed into non meaningful representations by linear taros.
- Better: Look at covariation of result correctness

}
}

@article{Lakshmanan:ve,
author = {Lakshmanan, B},
title = {{Transfer learning across heterogeneous robots with action sequence mapping}},
journal = {Intelligent Robots and Systems (IROS) {\ldots}},
note = {- Use of robots with diff capabilities
- Same States, 
- Diff Actions, Transitions
- New Pseudo Reward Func based on intermediary positions of successful agent 1
- Action sequence mapping: 
	- If a sequence of actions (1 or more) a* leads agent 1 from state S{\_}0 -{\&}gt;* S{\_}i and a sequence of actions b* leads agent 2 from state S{\_}0 -{\&}gt;* S{\_}i then a* {\&}lt;={\&}gt; b*
- Results are ok
- No guard against negative transfer
}
}

@article{Grollman:2011uf,
author = {Grollman, DH},
title = {{Teaching old dogs new tricks: Incremental multimap regression for interactive robot learning from demonstration}},
year = {2011}
}

@article{Rawlik:2010vh,
author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
title = {{Approximate Inference and Stochastic Optimal Control}},
journal = {arXiv.org},
year = {2010},
volume = {cs.LG},
month = sep
}

@article{Dayan:1997uo,
author = {Dayan, P},
title = {{Using expectation-maximization for reinforcement learning}},
journal = {Neural Computation},
year = {1997}
}

@article{Bagnell:2003tl,
author = {Bagnell, JA},
title = {{Covariant policy search}},
journal = {International Joint Conference on Artificial Intelligence},
year = {2003}
}

@article{Bagnell:2004ta,
author = {Bagnell, JA},
title = {{Learning decisions: Robustness, uncertainty, and approximation}},
year = {2004}
}

@inproceedings{Kakade:2002wk,
author = {Kakade, S},
title = {{A natural policy gradient}},
booktitle = {Advances in Neural In-formation Processing Systems (NIPS)},
year = {2002}
}

@article{Bagnell:tl,
author = {Bagnell, JA},
title = {{Policy Gradient Methods}},
journal = {kyb.tue.mpg.de}
}

@article{Bagnell:2003vq,
author = {Bagnell, JA and Kakade, S and Ng, A},
title = {{Policy search by dynamic programming}},
journal = {Neural information processing {\ldots}},
year = {2003}
}

@book{Maguire:1990fkTetherball,
title = {{Hopscotch, hangman, hot potato, and ha, ha,ha: a rulebook of children's games}},
publisher = {Simon and Schuster},
year = {1990}
}

@article{Rosenstein:2001tc,
author = {Rosenstein, MT},
title = {{Robot weightlifting by direct policy search}},
journal = {Proceedings of the International Joint Conference on Artificial Intelligence (ICJAI)},
year = {2001}
}

@inproceedings{Fox:1999ws,
author = {Fox, D and Burgard, W and Dellaert, F},
title = {{Monte carlo localization: Efficient position estimation for mobile robots}},
booktitle = {Proceedings of the National {\ldots}},
year = {1999}
}

@inproceedings{Shapiro:2001ef,
author = {Shapiro, Daniel and Langley, Pat and Shachter, Ross},
title = {{Proceedings of the fifth international conference on Autonomous agents - AGENTS '01}},
booktitle = {the fifth international conference},
year = {2001},
pages = {254--261},
publisher = {ACM Press},
address = {New York, New York, USA}
}

@article{Baxter:2000us,
author = {Baxter, J},
title = {{Reinforcement learning in POMDP's via direct gradient ascent}},
journal = {{\ldots} LEARNING-INTERNATIONAL WORKSHOP THEN {\ldots}},
year = {2000}
}

@inproceedings{Lawrence:2003ti,
author = {Lawrence, G and Cowan, N},
title = {{Efficient gradient estimation for motor control learning}},
booktitle = {Proceedings of the Nineteenth {\ldots}},
year = {2003}
}

@article{Toussaint:2006ve,
author = {Toussaint, M},
title = {{Probabilistic inference for solving discrete and continuous state Markov Decision Processes}},
journal = {Proceedings of the 23rd international conference on Machine Learning (ICML)},
year = {2006}
}

@article{Endo:2008en,
author = {Endo, G and Morimoto, J and Matsubara, T and Nakanishi, J and Cheng, G},
title = {{Learning CPG-based Biped Locomotion with a Policy Gradient Method: Application to a Humanoid Robot}},
journal = {International Journal of Robotics Research},
year = {2008},
volume = {27},
number = {2},
pages = {213--228},
month = feb
}

@article{Todorov:2005wq,
author = {Todorov, E and Li, W},
title = {{From task parameters to motor synergies: A hierarchical framework for approximately optimal control of redundant manipulators}},
journal = {Journal of Robotic Systems},
year = {2005}
}

@article{Gutfreund:2006ul,
author = {Gutfreund, Y and Matzner, H and Flash, T},
title = {{Patterns of Motor Activity in the Isolated Nerve Cord of the Octopus Arm}},
journal = {The Biological Bulletin},
year = {2006}
}

@article{Iba:1989wm,
author = {Iba, GA},
title = {{A heuristic approach to the discovery of macro-operators}},
journal = {Machine Learning},
year = {1989}
}

@article{Sutton:1998ce,
author = {{Sutton}},
title = {{Reinforcement Learning: An Introduction}},
journal = {IEEE Transactions on Neural Networks},
year = {1998},
volume = {9},
number = {5},
pages = {1054}
}

@webpage{PictureTetherball,
title = {{http://www.buylifetime.com/ products/ blt/ pid-90029.aspx}},
url = {http://www.buylifetime.com/products/blt/pid-90029.aspx}
}

@article{Mahadevan:1996wr,
author = {Mahadevan, S},
title = {{Average reward reinforcement learning: Foundations, algorithms, and empirical results}},
journal = {Machine Learning},
year = {1996},
volume = {22},
pages = {159--195}
}

@inproceedings{Neumann:2011wq,
author = {Neumann, Gerhard},
title = {{Variational Inference for Policy Search in changing situations}},
booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML)},
year = {2011},
pages = {817--824},
publisher = {ACM},
address = {New York, NY, USA},
month = jun
}

@article{Shen:2006ud,
author = {Shen, J and Gu, G},
title = {{IEEE Xplore - Multi-Agent Hierarchical Reinforcement Learning by Integrating Options into MAXQ}},
journal = {{\ldots}  IMSCCS'06 First International Multi- {\ldots}},
year = {2006}
}

@techreport{Schaal:2006tq,
author = {Schaal, Stefan},
title = {{The SL Simulation and Real-Time Control Software Package}},
year = {2006},
month = mar
}

@article{Kohl:2004we,
author = {Kohl, N},
title = {{Policy gradient reinforcement learning for fast quadrupedal locomotion}},
journal = {Proceedings of the Robotics and Automation (ICRA)},
year = {2004}
}

@inproceedings{Rohanimanesh:2005vj,
author = {Rohanimanesh, K},
title = {{Coarticulation}},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning (ICML)},
year = {2005}
}

@inproceedings{Kormushev:2010ig,
author = {Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G},
title = {{2010 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
year = {2010},
pages = {3232--3237},
publisher = {IEEE}
}

@article{Welch:1995tx,
author = {Welch, G},
title = {{An introduction to the Kalman filter}},
journal = {University of North Carolina at Chapel Hill},
year = {1995}
}

@article{Stone:2001tt,
author = {Stone, P},
title = {{Scaling reinforcement learning toward RoboCup soccer}},
journal = {Proceedings of the Eighteenth International Conference on Machine Learning (ICML), pp. 537--544, Morgan Kaufmann, San Francisco, CA, 2001.},
year = {2001}
}

@inproceedings{Deisenroth:2011vk,
author = {Deisenroth, MP},
title = {{PILCO: A model-based and data-efficient approach to policy search}},
booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML)},
year = {2011}
}

@inproceedings{Jong:2008hs,
author = {Jong, Nicholas K and Stone, Peter},
title = {{Proceedings of the 25th international conference on Machine learning - ICML '08}},
booktitle = {the 25th international conference},
year = {2008},
pages = {432--439},
publisher = {ACM Press},
address = {New York, New York, USA}
}

@article{Peters:2005uw,
author = {Peters, J and Vijayakumar, S},
title = {{Natural Actor-Critic}},
journal = {Proceedings of the European Conference on Machine Learning (ECML)},
year = {2005}
}

@article{Muelling:2010vz,
author = {Muelling, K and Kober, J},
title = {{Learning Table Tennis with a Mixture of Motor Primitives}},
journal = {Proceedings of the International Conference on Humanoid Robotics},
year = {2010}
}

@inproceedings{Peters:2006gw,
author = {Peters, Jan and Schaal, Stefan},
title = {{2006 IEEE/RSJ International Conference on Intelligent Robots and Systems}},
booktitle = {2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
year = {2006},
pages = {2219--2225},
publisher = {IEEE},
month = oct
}

@inproceedings{Ng:2000wr,
author = {Ng, AY},
title = {{PEGASUS: A policy search method for large MDPs and POMDPs}},
booktitle = {Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence (UAI)},
year = {2000},
publisher = {unknown}
}

@inproceedings{Morimoto:1998hj,
author = {Morimoto, J and Doya, K},
title = {{Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)}},
booktitle = {Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications},
year = {1998},
pages = {1721--1726},
publisher = {IEEE}
}

@article{Schaal:2005ur,
author = {Schaal, S and Peters, J and Nakanishi, J},
title = {{SpringerLink - Abstract}},
journal = {Robotics Research},
year = {2005}
}

@article{Sutton:1999wi,
author = {Sutton, RS and Precup, D},
title = {{Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning}},
journal = {Artificial intelligence},
year = {1999}
}

@article{Fox:1999uc,
author = {Fox, D and Burgard, W},
title = {{Markov localization for mobile robots in dynamic environments}},
journal = {Journal of Artificial Intelligence Research},
year = {1999}
}

@inproceedings{Bagnell:2001eu,
author = {Bagnell, J A and Schneider, J G},
title = {{Autonomous Helicopter Control using Reinforcement Learning Policy Search Methods}},
booktitle = {2001 ICRA. IEEE International Conference on Robotics and Automation},
pages = {1615--1620},
publisher = {IEEE}
}

@article{Still:2011vd,
author = {Still, Susanne and Precup, Doina},
title = {{An information-theoretic approach to curiosity-driven reinforcement learning}},
journal = {Proceedings of the International Conference on Humanoid Robotics},
year = {2011},
pages = {1--10},
month = feb
}

@article{Parr:1998vk,
author = {Parr, R},
title = {{Reinforcement learning with hierarchies of machines}},
journal = {Advances in Neural Information Processing Systems (NIPS)},
year = {1998}
}

@inproceedings{Mannor:2004jm,
author = {Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N},
title = {{Twenty-first international conference on Machine learning - ICML '04}},
booktitle = {Twenty-first international conference},
year = {2004},
pages = {72},
publisher = {ACM Press},
address = {New York, New York, USA}
}

@article{Baxter:2001tz,
author = {Baxter, J},
title = {{Infinite-horizon policy-gradient estimation}},
journal = {Journal of Artificial Intelligence Research (JAIR)},
year = {2001}
}

@article{Dietterich:1999th,
author = {Dietterich, Thomas G.},
title = {{Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition}},
year = {1999},
month = may
}

@article{Azar:2010uy,
author = {Azar, Mohammad Gheshlaghi and Gomez, Vicenc and Kappen, Hilbert J},
title = {{Dynamic Policy Programming}},
journal = {arXiv.org},
year = {2010},
volume = {cs.LG},
month = apr,
note = {Submitted to Journal of Machine Learning Research}
}

@article{Strens:2003wh,
author = {Strens, MJA},
title = {{Policy search using paired comparisons}},
journal = {The Journal of Machine Learning Research},
year = {2003}
}

@article{Sutton:1996vg,
author = {Sutton, RS},
title = {{Generalization in reinforcement learning: Successful examples using sparse coarse coding}},
journal = {Advances in Neural Information Processing Systems (NIPS)},
year = {1996}
}

@article{Bagnell:2006ve,
author = {Bagnell, D},
title = {{On local rewards and scaling distributed reinforcement learning}},
journal = {Advances in Neural Information Processing Systems},
year = {2006}
}

